import json
import logging
import re
import asyncio
from typing import List, Dict, Any, Set, Tuple
from bs4 import BeautifulSoup

# Import client classes (these should be available from the app directory)
from app.clients import PostgresClient, MSSQLClient
from app import confluence, llm

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import MCP framework (this should be available in the environment)
# Note: The actual import will depend on your MCP setup
# This is a placeholder that represents the MCP framework
import mcp

@mcp.tool()
async def get_confluence_page_content(space: str, title: str) -> str:
    """
    Fetches the HTML storage-format body of a Confluence page.

    Args:
      space (str): The Confluence space key (e.g. "PROJ").
      title (str): The title of the page to fetch.

    Returns:
      str: The storage-format HTML content of the page body.
    """

    # Resolve the page ID
    page_id = await confluence.get_page_id(space, title)
    # Fetch full page content (including storage body)
    page = await confluence.get_page_content(page_id, expand="body.storage")
    # Return the raw HTML/storage value
    return page["body"]["storage"]["value"]

@mcp.tool()
async def append_to_confluence_page(space: str, title: str, html_fragment: str) -> str:
    """
    Appends an HTML fragment to the end of an existing Confluence page.

    Args:
      space (str): The Confluence space key.
      title (str): The title of the page to update.
      html_fragment (str): A snippet of HTML (storage format) to append.

    Returns:
      str: Confirmation message including the page ID.
    """
    # Resolve the page ID
    page_id = await confluence.get_page_id(space, title)
    # Append the fragment and update the page
    await confluence.append_to_page(page_id, html_fragment)
    logger.info(f"Appended content to Confluence page '{title}' (ID: {page_id})")

@mcp.tool()
async def update_confluence_table(
    space: str,
    title: str,
    data: list[dict]
) -> dict:
    """
    Replace the contents of the first <table class="relative-table"> in a Confluence page
    with new rows generated from a list of column metadata.

    Args:
        space (str):    Confluence space key (e.g., "PROJ").
        title (str):    Title of the Confluence page to update.
        data (list[dict]):
            A list of dictionaries, each with keys:
                "column" (str):     Column identifier (e.g., "shops.id").
                "description" (str): Text description of the column.
                "type" (str):       Data type of the column (e.g., "integer", "varchar").
                "schema" (str):     Schema name where the table/view resides.
                "owner" (str):      Owner information (optional, defaults to empty).

    Returns:
        dict: The full response from Confluence API's update_page call, containing
              updated page metadata and version information.

    Raises:
        ValueError: If the target table (<table class="relative-table">) is not found on the page.
        Exception: Propagates any errors from the Confluence API client.

    Example:
        >>> new_rows = [
        ...     {
        ...         "column": "shops.id", 
        ...         "description": "Unique shop ID",
        ...         "type": "integer",
        ...         "schema": "public",
        ...         "owner": ""
        ...     }
        ... ]
        >>> updated = await update_confluence_table("MALL", "Shop Catalog", new_rows)
        >>> print(updated["version"]["number"])
    """
    logger.info("üîç Updating Confluence table in %s/%s with %d rows", space, title, len(data))
    
    # 1) Fetch existing page content in storage format (HTML)
    page_id = await confluence.get_page_id(space, title)
    page = await confluence.get_page_content(page_id, expand="body.storage,version")
    html = page["body"]["storage"]["value"]

    # 2) Parse with BeautifulSoup and locate or create the target table
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="relative-table")
    
    if not table:
        logger.info("üìã No existing table found, creating new table structure")
        # Create new table if none exists
        table = soup.new_tag("table", **{"class": "relative-table"})
        tbody = soup.new_tag("tbody")
        
        # Create header row
        header_row = soup.new_tag("tr")
        headers = ["Name", "Description", "Type", "Schema", "Owner"]
        for header_text in headers:
            th = soup.new_tag("th")
            th.string = header_text
            header_row.append(th)
        tbody.append(header_row)
        
        table.append(tbody)
        
        # Add table to the end of the page content
        if soup.body:
            soup.body.append(table)
        else:
            soup.append(table)
        
        logger.info("‚úÖ Created new table with 5-column structure")
    else:
        logger.info("üìã Found existing table, updating structure if needed")
        # Check if table has the right number of columns in header
        header_row = table.find("tr")
        if header_row:
            headers = header_row.find_all(["th", "td"])
            if len(headers) < 5:
                logger.info("üîÑ Updating table header to 5-column structure")
                # Clear existing headers and rebuild
                header_row.clear()
                header_texts = ["Name", "Description", "Type", "Schema", "Owner"]
                for header_text in header_texts:
                    th = soup.new_tag("th")
                    th.string = header_text
                    header_row.append(th)

    # 2a) Remove all old rows except the header
    rows = table.find_all("tr")
    for old_row in rows[1:]:
        old_row.extract()

    # 2b) Append new rows from 'data'
    for entry in data:
        col_val = entry.get("column", "")
        desc_val = entry.get("description", "")
        type_val = entry.get("type", "")
        schema_val = entry.get("schema", "")
        owner_val = entry.get("owner", "")
        
        new_tr = soup.new_tag("tr")
        
        # Create all 5 columns
        td_col = soup.new_tag("td"); td_col.string = col_val
        td_desc = soup.new_tag("td"); td_desc.string = desc_val
        td_type = soup.new_tag("td"); td_type.string = type_val
        td_schema = soup.new_tag("td"); td_schema.string = schema_val
        td_owner = soup.new_tag("td"); td_owner.string = owner_val
        
        new_tr.extend([td_col, td_desc, td_type, td_schema, td_owner])
        table.tbody.append(new_tr)

    # 2c) Serialize modified HTML back to a string
    updated_html = str(soup)
    logger.debug("Updated HTML length: %d characters", len(updated_html))

    # 3) Push update via Confluence API (auto-bumps version)
    updated = await confluence.update_page(
        page_id,
        title,
        updated_html,
        minor_edit=True
    )
    
    logger.info("‚úÖ Successfully updated Confluence table with %d rows", len(data))
    return updated

@mcp.tool()
async def sync_confluence_table_delta(
    space: str,
    title: str,
    data: list[dict]
) -> dict:
    """
    Read the existing <table class="relative-table"> from a Confluence page,
    compute which entries in `data` are not already present (by column key),
    append only those delta rows to the table, and push the update.

    Args:
        space (str):    Confluence space key (e.g., "PROJ").
        title (str):    Title of the Confluence page to update.
        data (list[dict]):
            List of dicts with keys "column", "description", "type", "schema", "owner".

    Returns:
        dict: Full Confluence API response for the update_page call, or
              an empty dict if no delta rows to append.

    Raises:
        ValueError: If the target table (<table class="relative-table">) is not found.
        Exception: Propagates errors from the Confluence client.
    """
    logger.info("üîç sync_confluence_table_delta called for %s/%s", space, title)
    logger.info("üìä Received %d data entries to process", len(data))
    
    # Log the structure of received data for debugging
    if data:
        logger.info("üìã Sample data entry: %s", data[0])
        logger.info("üìã Data entry keys: %s", list(data[0].keys()) if data else "No data")
    else:
        logger.warning("‚ö†Ô∏è  No data provided to sync_confluence_table_delta - will create empty table if none exists")
    
    logger.info("üîç Syncing Confluence table delta in %s/%s with %d potential rows", 
               space, title, len(data))
    
    # 1) Fetch current page HTML and version
    page_id = await confluence.get_page_id(space, title)
    page = await confluence.get_page_content(page_id, expand="body.storage,version")
    html = page["body"]["storage"]["value"]

    # 2) Parse HTML and extract existing keys or create table if needed
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="relative-table")
    
    if not table:
        logger.info("üìã No existing table found, creating new table structure")
        # Create new table if none exists
        table = soup.new_tag("table", **{"class": "relative-table"})
        tbody = soup.new_tag("tbody")
        
        # Create header row with 5 columns
        header_row = soup.new_tag("tr")
        headers = ["Name", "Description", "Type", "Schema", "Owner"]
        for header_text in headers:
            th = soup.new_tag("th")
            th.string = header_text
            header_row.append(th)
        tbody.append(header_row)
        
        table.append(tbody)
        
        # Add table to the end of the page content
        if soup.body:
            soup.body.append(table)
        else:
            soup.append(table)
        
        existing = set()  # No existing keys since table was just created
        logger.info("‚úÖ Created new table with 5-column structure")
        
        # If no data provided, just create the table and return
        if not data:
            updated_html = str(soup)
            updated = await confluence.update_page(
                page_id,
                title,
                updated_html,
                minor_edit=True
            )
            logger.info("‚úÖ Created empty table structure on Confluence page")
            return {"delta": [], "updated": updated, "message": "Created empty table structure"}
    else:
        # Extract existing keys from first <td> in each <tr> (skip header)
        existing = set()
        rows = table.find_all('tr')
        
        # Check and update header if needed
        if rows:
            header_row = rows[0]
            headers = header_row.find_all(["th", "td"])
            if len(headers) < 5:
                logger.info("üîÑ Updating table header to 5-column structure")
                header_row.clear()
                header_texts = ["Name", "Description", "Type", "Schema", "Owner"]
                for header_text in header_texts:
                    th = soup.new_tag("th")
                    th.string = header_text
                    header_row.append(th)
        
        # Extract existing keys from data rows
        for tr in rows[1:]:
            td = tr.find('td')
            if td and td.text:
                existing.add(td.text.strip())

    # Handle case where no data provided but table exists
    if not data:
        logger.info("‚úÖ No data to sync, but table structure is confirmed to exist")
        return {"delta": [], "updated": None, "message": "No data to sync"}

    logger.debug("üìã Existing keys in table: %s", existing)
    logger.debug("üìã Incoming data columns: %s", [item.get("column", "<missing>") for item in data])

    # 3) Determine delta rows
    delta = [entry for entry in data if entry.get("column", "") not in existing]
    
    logger.info("üìä Delta analysis: %d existing keys, %d incoming items, %d delta rows", 
               len(existing), len(data), len(delta))
    
    if not delta:
        logger.info("‚úÖ No delta rows to add - table is up to date")
        if not existing and data:
            logger.warning("‚ö†Ô∏è  Strange: no existing keys but incoming data didn't create delta. Data format issue?")
            logger.debug("üîç First data item structure: %s", data[0] if data else "None")
        return {"delta": [], "updated": None, "message": "No new columns to sync"}
    
    logger.info("üìä Found %d delta rows to add", len(delta))
    logger.debug("üìã Delta columns: %s", [item.get("column", "<missing>") for item in delta])

    # 4) Append delta rows to the table in the soup
    for entry in delta:
        col_val = entry.get("column", "")
        desc_val = entry.get("description", "")
        type_val = entry.get("type", "")
        schema_val = entry.get("schema", "")
        owner_val = entry.get("owner", "")
        
        new_tr = soup.new_tag("tr")
        
        # Create all 5 columns
        td_col = soup.new_tag("td"); td_col.string = col_val
        td_desc = soup.new_tag("td"); td_desc.string = desc_val
        td_type = soup.new_tag("td"); td_type.string = type_val
        td_schema = soup.new_tag("td"); td_schema.string = schema_val
        td_owner = soup.new_tag("td"); td_owner.string = owner_val
        
        new_tr.extend([td_col, td_desc, td_type, td_schema, td_owner])
        table.tbody.append(new_tr)

    updated_html = str(soup)

    # 5) Push update via Confluence API (auto-version)
    updated = await confluence.update_page(
        page_id,
        title,
        updated_html,
        minor_edit=True
    )

    logger.info("‚úÖ Successfully synced %d delta rows to Confluence table", len(delta))
    return {
        "delta":   delta,   # list of {column,description,type,schema,owner}
        "updated": updated  # full API response
    }

@mcp.tool()
async def get_confluence_page_id(
    space: str,
    title: str
) -> str:
    """
    Retrieve the numeric ID of a Confluence page by space and title.
    """
    logger.debug("get_confluence_page_id called with space=%s, title=%s", space, title)
    page_id = await asyncio.to_thread(
        confluence.get_page_id,
        space,
        title
    )
    logger.debug("get_confluence_page_id result: page_id=%s", page_id)
    return page_id

@mcp.tool()
async def post_confluence_comment(
    space: str,
    title: str,
    comment: str
) -> Dict[str, Any]:
    """
    Add a comment to a Confluence page by ID.
    Returns the Confluence API response.
    """
    page_id = await confluence.get_page_id(space=space,title=title)
    logger.debug("post_confluence_comment called with page_id=%s, comment=%s", page_id, comment)
    resp = await confluence.post_comment(page_id,comment)
    logger.debug("post_confluence_comment result: %s", resp)
    return resp

@mcp.tool()
async def collect_db_confluence_key_descriptions(
    space: str,
    title: str,
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
) -> Dict[str, str]:
    """
    Return a mapping of keys that exist BOTH in Confluence and in the DB schema,
    with their descriptions taken from Confluence.
    """
    # --- helpers for safe logging ---
    def _mask_secret(s: str, show: int = 2) -> str:
        if s is None:
            return "None"
        s = str(s)
        return (s[:show] + "..." + s[-show:]) if len(s) > (show * 2) else "***"

    def _sample_dict(d: Dict[str, str], n: int = 15) -> Dict[str, str]:
        keys = list(d.keys())[:n]
        return {k: d[k] for k in keys}

    def _sample_schema(schema: Dict[str, List[str]], n_tables: int = 10, n_cols: int = 10) -> Dict[str, List[str]]:
        out = {}
        for i, (tbl, cols) in enumerate(schema.items()):
            if i >= n_tables:
                break
            out[tbl] = cols[:n_cols]
        return out

    try:
        logger.info(
            "collect_db_confluence_key_descriptions: start space=%r title=%r host=%s port=%s db=%s user=%s",
            space, title, host, port, database, user
        )

        # --- 1) Confluence: read storage HTML and pull key->desc from the table ---
        logger.debug("Confluence: resolving page_id for space=%r title=%r ‚Ä¶", space, title)
        page_id = await confluence.get_page_id(space, title)
        logger.info("Confluence: got page_id=%s", page_id)

        logger.debug("Confluence: fetching page content expand='body.storage,version' ‚Ä¶")
        page = await confluence.get_page_content(page_id, expand="body.storage,version")
        html = page["body"]["storage"]["value"]
        logger.debug("Confluence: storage HTML length=%d chars", len(html))

        soup = BeautifulSoup(html, "html.parser")
        table = soup.find("table", class_="relative-table")
        if not table:
            logger.error("Confluence: <table class='relative-table'> not found on page=%r space=%r", title, space)
            raise ValueError(f"relative-table not found on page '{title}' in space '{space}'")

        rows = table.find_all("tr")
        logger.debug("Confluence: found %d <tr> rows in relative-table", len(rows))

        conf_map: Dict[str, str] = {}
        for tr in rows[1:]:  # skip header
            tds = tr.find_all("td")
            if not tds:
                continue
            key = tds[0].get_text(" ", strip=True) if len(tds) >= 1 else ""
            desc = tds[1].get_text(" ", strip=True) if len(tds) >= 2 else ""
            if key:
                conf_map[key] = desc

        logger.info("Confluence: parsed %d key(s) from table", len(conf_map))
        logger.debug("Confluence: sample key‚Üídesc: %s", _sample_dict(conf_map))

        # --- 2) DB schema: build canonical key set and helpers ---
        logger.info("DB: connecting to %s:%s db=%s user=%s", host, port, database, user)
        pg = PostgresClient(host=host, port=port, user=user, password=password, database=database)
        await pg.init()
        try:
            logger.debug("DB: calling list_keys() ‚Ä¶")
            schema: Dict[str, List[str]] = await pg.list_keys()  # {table: [col,...]}
        finally:
            logger.debug("DB: closing connection pool ‚Ä¶")
            await pg.close()

        total_tables = len(schema or {})
        total_cols = sum(len(v) for v in (schema or {}).values())
        logger.info("DB: schema loaded tables=%d total_columns=%d", total_tables, total_cols)
        logger.debug("DB: sample schema: %s", _sample_schema(schema or {}))

        db_full_keys: Set[str] = set()
        lower_to_canonical: Dict[str, str] = {}
        col_to_tables: Dict[str, Set[str]] = {}

        for tbl, cols in (schema or {}).items():
            for col in cols:
                full = f"{tbl}.{col}"
                db_full_keys.add(full)
                lower_to_canonical[full.lower()] = full
                col_to_tables.setdefault(col, set()).add(tbl)
                col_to_tables.setdefault(col.lower(), set()).add(tbl)

        logger.debug(
            "DB: canonical keys built count=%d unique_columns=%d",
            len(db_full_keys), len({c for c in col_to_tables.keys() if isinstance(c, str) and c.islower()})
        )

        # --- 3) Intersect: keep keys that exist in both sources, prefer canonical table.col ---
        result: Dict[str, str] = {}
        matched_exact = 0
        matched_by_unique_column = 0
        skipped_ambiguous = 0

        for raw_key, desc in conf_map.items():
            k = (raw_key or "").strip()
            if not k:
                continue

            # a) Exact table.column form (case-insensitive)
            if "." in k:
                canon = lower_to_canonical.get(k.lower())
                if canon:
                    result[canon] = desc
                    matched_exact += 1
                continue

            # b) Column-only form: include only if column is unique across all tables
            candidates = col_to_tables.get(k) or col_to_tables.get(k.lower()) or set()
            if len(candidates) == 1:
                only_tbl = next(iter(candidates))
                canon = lower_to_canonical.get(f"{only_tbl}.{k}".lower())
                if canon:
                    result[canon] = desc
                    matched_by_unique_column += 1
            else:
                if candidates:
                    skipped_ambiguous += 1  # present in multiple tables

        logger.info(
            "Intersect: result=%d (exact=%d, unique-col=%d, ambiguous-skipped=%d)",
            len(result), matched_exact, matched_by_unique_column, skipped_ambiguous
        )
        logger.debug("Intersect: sample result: %s", _sample_dict(result))
        return result

    except Exception as e:
        logger.exception(
            "collect_db_confluence_key_descriptions failed for space=%r title=%r host=%s port=%s db=%s user=%s err=%s",
            space, title, host, port, database, user, e
        )
        raise


@mcp.tool()
async def list_databases(
    host: str,
    port: int,
    user: str,
    password: str,
    database_type: str = "postgres"
) -> List[str]:
    """
    Retrieve all database names from a server, for Postgres or MSSQL.

    This tool connects to the specified host/port with the given credentials,
    then lists all non-system databases. For Postgres it uses the "postgres"
    maintenance database; for MSSQL it uses "master".

    Args:
      host (str):          IP or hostname of the DB server.
      port (int):          TCP port (Postgres default 5432, MSSQL default 1433).
      user (str):          Username with list-permissions.
      password (str):      Password for the user.
      database_type (str): Either "postgres" or "mssql".

    Returns:
      List[str]:           List of database names.

    Raises:
      ValueError:
        If `database_type` is unsupported.
      asyncpg.PostgresError:
        On any Postgres connection or query failure.
      mssql_python.Error:
        On any MSSQL connection or query failure.  # DB-API base exception :contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}
    """
    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database="postgres", min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database="master")
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    logger.info("Connected to %s at %s:%s as %s", database_type, host, port, user)

    try:
        dbs = await client.list_databases()
        logger.info("Databases found: %s", dbs)
        return dbs
    finally:
        await client.close()


@mcp.tool()
async def list_database_schemas(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    database_type: str = "postgres"
) -> List[str]:
    """
    List all schemas in the specified database, for Postgres or MSSQL.

    This tool connects to the specified database and returns all user-defined
    schemas, excluding system schemas like information_schema, pg_catalog, etc.

    Args:
      host (str):          IP or hostname of the DB server.
      port (int):          TCP port (Postgres default 5432, MSSQL default 1433).
      user (str):          Username with list-permissions.
      password (str):      Password for the user.
      database (str):      Database name to inspect for schemas.
      database_type (str): Either "postgres" or "mssql".

    Returns:
      List[str]:           List of schema names.

    Raises:
      ValueError:
        If `database_type` is unsupported.
      asyncpg.PostgresError:
        On any Postgres connection or query failure.
      mssql_python.Error:
        On any MSSQL connection or query failure.
    """
    logger.info(
        "list_database_schemas called against %s:%s/%s as %s (%s)",
        host, port, database, user, database_type
    )

    if database_type == "postgres":
        client = PostgresClient(
            host, port, user, password,
            database=database, min_size=1, max_size=5
        )
    elif database_type == "mssql":
        client = MSSQLClient(
            host, port, user, password,
            database=database
        )
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        schemas_list: List[str] = await client.list_schemas()
        logger.debug("Raw schemas_list for %s: %r", database_type, schemas_list)
        return schemas_list
    except Exception as e:
        logger.error(
            "‚ùå list_database_schemas failed for %s://%s:%s/%s as %s",
            database_type, host, port, database, user,
            exc_info=True
        )
        raise
    finally:
        await client.close()


@mcp.tool()
async def list_database_tables(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str = "",
    database_type: str = "postgres"
) -> str:
    """
    List all user tables and views in the specified database, for Postgres or MSSQL.

    Returns:
      A JSON‚Äêencoded array of table and view names, e.g. '["shops","items","sales","customer_view"]'.

    Raises:
      ValueError: If `database_type` is unsupported.
      Otherwise, re-raises any DB client errors so you can see them.
    """
    logger.info(
        "üîç list_database_tables called against %s:%s/%s as %s (%s) - including tables and views",
        host, port, database or "<default>", user, database_type
    )

    # 1) Instantiate the right client
    if database_type == "postgres":
        client = PostgresClient(
            host, port, user, password,
            database=database or "postgres", min_size=1, max_size=5
        )
    elif database_type == "mssql":
        client = MSSQLClient(
            host, port, user, password,
            database=database or "master"
        )
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    # 2) Open
    await client.init()

    try:
        # 3) Fetch raw list
        tables_list: List[str] = await client.list_tables()
        logger.info("üìã Retrieved %d database objects (tables and views) for %s", 
                   len(tables_list), database_type)
        logger.debug("Raw objects list for %s: %r", database_type, tables_list)

        # 4) Serialize
        tables_json = json.dumps(tables_list)
        logger.debug("Returning tables_json: %s", tables_json)

        return tables_json

    except Exception as e:
        # 5) Log full traceback so you see *why* MSSQL is failing
        logger.error(
            "‚ùå list_database_tables failed for %s://%s:%s/%s as %s",
            database_type, host, port, database or "<default>", user,
            exc_info=True
        )
        # Re-raise so the MCP server surfaces the error
        raise

    finally:
        # 6) Always close
        await client.close()


@mcp.tool()
async def list_database_keys(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    database_type: str = "postgres"
) -> Dict[str, List[str]]:
    """
    List all column names (‚Äúkeys‚Äù) for each table in the specified database,
    for Postgres or MSSQL.

    Steps:
      1. Initialize the appropriate client.
      2. Call client.list_keys().
      3. Close the pool.
      4. Return the mapping.

    Args:
      host (str):          DB host.
      port (int):          DB port.
      user (str):          Username.
      password (str):      Password.
      database (str):      Database to inspect.
      database_type (str): Either "postgres" or "mssql".

    Returns:
      Dict[str, List[str]]:
        e.g. {
          "shops": ["shop_id", "name", ...],
          "items": ["item_id", "shop_id", ...]
        }

    Raises:
      ValueError:
        If `database_type` is unsupported.
      asyncpg.PostgresError:
        On any Postgres connection or query failure.
      mssql_python.Error:
        On any MSSQL connection or query failure.  # DB-API base exception :contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}
    """
    logger.info(
        "üîç list_database_keys called against %s:%s/%s as %s (%s) - including tables and views",
        host, port, database, user, database_type
    )

    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        keys_map = await client.list_keys()
        logger.info("üìã Retrieved column mappings for %d database objects (tables and views)", len(keys_map))
        logger.debug("Keys map: %s", keys_map)
        return keys_map
    finally:
        await client.close()

@mcp.tool()
async def get_database_column_metadata(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    schema: str = None,
    database_type: str = "postgres"
) -> Dict[str, Dict[str, Any]]:
    """
    Get detailed metadata for all columns including data types and schema information.
    
    Returns a mapping where each key is "schema.table.column" and value contains:
    - table_name, column_name, data_type, table_schema, is_nullable, etc.
    
    Args:
      host (str):          DB host.
      port (int):          DB port.
      user (str):          Username.
      password (str):      Password.
      database (str):      Database to inspect.
      schema (str):        Optional schema filter. If None, includes all user schemas.
      database_type (str): Either "postgres" or "mssql".
    
    Returns:
      Dict[str, Dict[str, Any]]: Comprehensive column metadata
    """
    logger.info(
        "üîç get_database_column_metadata called against %s:%s/%s as %s (%s) - schema filter: %s",
        host, port, database, user, database_type, schema or "ALL"
    )

    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        metadata = await client.get_column_metadata(schema=schema)
        logger.info("üìã Retrieved metadata for %d columns", len(metadata))
        logger.debug("Sample metadata keys: %s", list(metadata.keys())[:5])
        return metadata
    finally:
        await client.close()


@mcp.tool()
async def get_enhanced_schema_with_confluence(
    space: str,
    title: str,
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    columns: List[str],
    database_type: str = "postgres"
) -> Dict[str, List[Dict[str, str]]]:
    """
    Create enhanced schema structure ONLY for the specified columns.
    
    Takes a list of specific columns (table.column format) and returns only those
    with descriptions from Confluence and types from database.
    
    Args:
        space (str): Confluence space
        title (str): Confluence page title  
        host (str): Database host
        port (int): Database port
        user (str): Database username
        password (str): Database password
        database (str): Database name
        columns (List[str]): Specific columns to fetch (e.g., ["shops.id", "shops.name"])
        database_type (str): Either "postgres" or "mssql"
    
    Returns:
        Dict[str, List[Dict[str, str]]]: 
        {
            "schema.table": [
                {"name": "column", "description": "...", "type": "varchar"}, 
                ...
            ],
            ...
        }
    """
    logger.info("üîç get_enhanced_schema_with_confluence called for %d specific columns", len(columns))
    logger.debug("üìã Requested columns: %s", columns[:10])  # Log first 10 columns
    
    # 1. Get database metadata for ALL columns (we'll filter later)
    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        # Get detailed column metadata with types
        column_metadata = await client.get_column_metadata()
        logger.info("üìä Got metadata for %d total columns from database", len(column_metadata))
        
    finally:
        await client.close()
    
    # 2. Get Confluence descriptions for the specific columns
    logger.info("üìã Fetching Confluence descriptions from %s/%s", space, title)
    try:
        confluence_descriptions = await collect_db_confluence_key_descriptions(
            space=space,
            title=title,
            host=host,
            port=port,
            user=user,
            password=password,
            database=database
        )
        logger.info("‚úÖ Got %d descriptions from Confluence", len(confluence_descriptions))
    except Exception as e:
        logger.warning("‚ö†Ô∏è Failed to get Confluence descriptions: %s", e)
        confluence_descriptions = {}
    
    # 3. Build enhanced schema structure ONLY for requested columns
    enhanced_schema = {}
    processed_columns = 0
    
    def _normalize_column_spec(column_spec: str) -> str:
        """Normalize column spec to table.column format in case schema.table.column is passed."""
        if not column_spec or "." not in column_spec:
            return column_spec
            
        parts = column_spec.split(".")
        if len(parts) == 2:
            # Already in table.column format
            return column_spec
        elif len(parts) == 3:
            # schema.table.column format - return table.column
            logger.debug("Normalizing column spec: %s -> %s.%s", column_spec, parts[1], parts[2])
            return f"{parts[1]}.{parts[2]}"
        else:
            # Unexpected format - return as-is
            logger.warning("Unexpected column spec format: %s", column_spec)
            return column_spec
    
    for column_spec in columns:
        # Normalize the column specification
        normalized_spec = _normalize_column_spec(column_spec)
        
        if "." not in normalized_spec:
            logger.warning("‚ö†Ô∏è Invalid column format '%s' (original: '%s') - skipping", normalized_spec, column_spec)
            continue
            
        table_name = normalized_spec.split(".", 1)[0]
        column_name = normalized_spec.split(".", 1)[1]
        
        # Find the schema and metadata for this specific column
        table_schema = "public"  # default
        data_type = "UNKNOWN"
        
        # Look for this column in database metadata
        for meta_key, meta_data in column_metadata.items():
            if (meta_data["table_name"] == table_name and 
                meta_data["column_name"] == column_name):
                table_schema = meta_data["table_schema"]
                data_type = meta_data["data_type"]
                break
        
        schema_table_key = f"{table_schema}.{table_name}"
        
        # Get description from Confluence for this specific column
        description = confluence_descriptions.get(normalized_spec, "")
        
        # Create the column entry
        column_entry = {
            "name": column_name,
            "description": description,
            "type": data_type
        }
        
        # Add to enhanced schema
        if schema_table_key not in enhanced_schema:
            enhanced_schema[schema_table_key] = []
        
        enhanced_schema[schema_table_key].append(column_entry)
        processed_columns += 1
        
        logger.debug("‚úÖ Processed %s -> schema=%s, type=%s, desc_len=%d", 
                    normalized_spec, table_schema, data_type, len(description))
    
    logger.info("‚úÖ Built enhanced schema for %d columns across %d schema.table entries", 
               processed_columns, len(enhanced_schema))
    
    # Log sample for debugging  
    if enhanced_schema:
        sample_key = list(enhanced_schema.keys())[0]
        sample_columns = enhanced_schema[sample_key][:2]  # First 2 columns
        logger.debug("üìã Sample enhanced schema entry '%s': %s", sample_key, sample_columns)
    
    return enhanced_schema


@mcp.tool()
async def generate_column_data_for_confluence(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    columns: List[str],
    database_type: str = "postgres"
) -> List[Dict[str, Any]]:
    """
    Generate a complete data structure for Confluence sync from a list of column names.
    
    This function takes a simple list of column names (e.g., ["table.column1", "table.column2"])
    and returns a complete structure with descriptions, types, schemas, and empty owner fields
    ready for Confluence table synchronization.
    
    Args:
        host (str): Database host
        port (int): Database port  
        user (str): Database username
        password (str): Database password
        database (str): Database name
        columns (List[str]): List of column names in "table.column" format
        database_type (str): Either "postgres" or "mssql"
    
    Returns:
        List[Dict[str, Any]]: List of complete column data structures with:
            - column: "table.column"
            - description: Generated description (empty if generation fails)
            - type: Data type from database metadata
            - schema: Schema name from database metadata  
            - owner: Empty string (to be filled later)
    """
    logger.info("üîç generate_column_data_for_confluence called for %d columns", len(columns))
    logger.debug("üìã Input columns: %s", columns[:5] + ["..."] if len(columns) > 5 else columns)
    
    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        # Get comprehensive metadata for all columns
        metadata = await client.get_column_metadata()
        logger.info("üìä Retrieved metadata for %d total columns in database", len(metadata))
        
        result = []
        for col_spec in columns:
            try:
                # Parse table.column format
                if "." not in col_spec:
                    logger.warning("‚ö†Ô∏è  Column spec '%s' missing table prefix, skipping", col_spec)
                    continue
                    
                table_name = col_spec.split(".", 1)[0]
                column_name = col_spec.split(".", 1)[1]
                
                # Find matching metadata entry
                metadata_entry = None
                column_schema = "public"  # default
                data_type = "unknown"
                
                # Try to find exact match with schema
                for meta_key, meta_data in metadata.items():
                    if (meta_data["table_name"] == table_name and 
                        meta_data["column_name"] == column_name):
                        metadata_entry = meta_data
                        column_schema = meta_data["table_schema"]
                        data_type = meta_data["data_type"]
                        break
                
                if not metadata_entry:
                    logger.warning("‚ö†Ô∏è  No metadata found for column %s", col_spec)
                    column_schema = "public"
                    data_type = "unknown"
                
                # Create complete entry structure
                entry = {
                    "column": col_spec,
                    "description": "",  # Will be filled by describe_columns if needed
                    "type": data_type,
                    "schema": column_schema,
                    "owner": ""
                }
                
                result.append(entry)
                logger.debug("‚úÖ Processed %s: type=%s, schema=%s", col_spec, data_type, column_schema)
                
            except Exception as e:
                logger.error("‚ùå Error processing column %s: %s", col_spec, e)
                # Add entry with minimal info
                result.append({
                    "column": col_spec,
                    "description": f"Error: {e}",
                    "type": "unknown",
                    "schema": "unknown", 
                    "owner": ""
                })
        
        logger.info("‚úÖ Generated %d complete column data entries", len(result))
        return result
        
    finally:
        await client.close()


@mcp.tool()
async def get_table_delta_keys(
    space: str,
    title: str,
    columns: List[str]
) -> str:
    """
    Return the subset of `columns` that are not present in the
    <table class="relative-table"> on the Confluence page, as a JSON array.
    
    If no table exists on the page, returns all columns (since they're all "missing").

    Args:
      space (str):     Confluence space key (e.g., "PROJ").
      title (str):     Page title.
      columns (List[str]):
        List of column names (e.g. ["shops.id","shops.name", ‚Ä¶]).

    Returns:
      str: JSON‚Äêencoded list of column names that do NOT already appear
           as the first <td> in any row of the existing table, for example:
           '["sales.channel","sales.transaction_ref","sales.loyalty_points","sales.notes"]'.
           If no table exists, returns all input columns.

    Raises:
      Exception: Propagates errors from the Confluence client.
    """
    page_id = await confluence.get_page_id(space, title)
    page = await confluence.get_page_content(page_id, expand="body.storage")
    html = page["body"]["storage"]["value"]

    soup = BeautifulSoup(html, "html.parser")
    tbl = soup.find("table", class_="relative-table")
    if not tbl:
        # No table exists yet - all columns are "missing" and need to be added
        logger.info("üìã No existing table found on page '%s' - all %d columns are new", title, len(columns))
        return json.dumps(columns)

    existing = {
        tr.find("td").text.strip()
        for tr in tbl.find_all("tr")[1:]
        if tr.find("td") and tr.find("td").text
    }
    delta = [c for c in columns if c not in existing]

    logger.debug("Confluence columns: %s", existing)
    logger.debug("Database columns:    %s", columns)
    logger.debug("Delta columns:       %s", delta)

    # Return the full delta array as one JSON-encoded string
    return json.dumps(delta)

@mcp.tool()
async def suggest_keys_for_analytics(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    system_prompt: str,
    user_prompt: str
) -> str:
    """
    Suggests which table columns (‚Äúkeys‚Äù) BI developers should use for a given analytics request.

    1. Connects to the specified Postgres database.
    2. Retrieves all tables and their column names (list_keys).
    3. Calls the remote LLM with:
       ‚Ä¢ system_prompt: guidance that ‚ÄúYou are a BI assistant‚Ä¶‚Äù
       ‚Ä¢ user_prompt: the analytics question (e.g., ‚ÄúShow me top-selling items‚Ä¶‚Äù)
       ‚Ä¢ context: JSON of { table: [columns,‚Ä¶], ‚Ä¶ }
    4. Returns the LLM‚Äôs recommendation as a single string.

    Args:
      host (str):         Postgres host/IP.
      port (int):         Postgres port (5432).
      user (str):         DB username.
      password (str):     DB password.
      database (str):     DB name (e.g., ‚Äúmalldb‚Äù).
      system_prompt (str):
         Instruction to the LLM, e.g.
         ‚ÄúYou are a BI assistant. Given table schemas, pick the
          columns needed to answer the user‚Äôs query.‚Äù
      user_prompt (str):  The actual analytics request from the user.

    Returns:
      str: The LLM‚Äôs answer, listing the relevant keys.

    Raises:
      asyncpg.PostgresError: On DB connection/query errors.
      HTTPError:            On LLM API failures.
    """
    # 1) Fetch schema keys and metadata
    logger.info("üîç suggest_keys_for_analytics called for database %s:%s/%s", 
               host, port, database)
    
    pg = PostgresClient(
        host=host, port=port,
        user=user, password=password,
        database=database,
        min_size=1, max_size=5
    )
    await pg.init()
    try:
        # Get basic table->columns mapping
        keys_map: Dict[str, List[str]] = await pg.list_keys()
        
        # Get detailed column metadata for schema context
        metadata = await pg.get_column_metadata()
        
        logger.info("üìä Retrieved %d tables and %d column metadata entries", 
                   len(keys_map), len(metadata))
        
        # Build schema-aware context
        schema_context = {}
        for table, columns in keys_map.items():
            # Find schema for this table by looking at metadata
            table_schema = "public"  # default
            for meta_key, meta_data in metadata.items():
                if meta_data["table_name"] == table:
                    table_schema = meta_data["table_schema"]
                    break
            
            # Use schema.table as key for better context
            schema_table_key = f"{table_schema}.{table}"
            schema_context[schema_table_key] = columns
        
        logger.debug("Schema-aware context built: %d schema.table entries", len(schema_context))
        
    finally:
        await pg.close()

    # 2) Prepare enhanced context for LLM with schema information
    context_parts = [
        f"PostgreSQL database '{database}' schema information with schema qualifiers:",
        json.dumps(schema_context, indent=2),
        "",
        f"IMPORTANT: All tables are within the SINGLE database named '{database}'.",
        "The format 'schema.table' refers to SCHEMA.TABLE within the same database.",
        "When suggesting columns, use format 'schema.table.column' (e.g., 'public.users.id').",
        "Do NOT suggest database.table.column format - only schema.table.column.",
        "All schemas shown are within the same PostgreSQL database."
    ]
    context = "\n".join(context_parts)

    # 3) Invoke LLM with enhanced context
    logger.info("üì§ Calling LLM with schema-aware context (%d chars)", len(context))
    recommendation = await llm.call_remote_llm(
        context=context,
        prompt=user_prompt,
        system_prompt=system_prompt
    )

    logger.info("‚úÖ LLM recommendation received (%d chars)", len(recommendation or ""))
    logger.debug("LLM recommendation preview: %s", (recommendation or "")[:200])
    return recommendation


@mcp.tool()
async def run_analytics_query_on_database(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    analytics_prompt: str,
    system_prompt: str
) -> List[Dict[str, Any]]:
    """
    1. Introspect tables+columns.
    2. Ask LLM to generate a JOIN/aggregation SQL.
    3. Execute that SQL, return the resulting rows.
    """
    def _strip_sql_fences(txt: str) -> str:
        if not txt:
            return txt
        t = re.sub(r"</?code[^>]*>", "", txt, flags=re.I)
        m = re.search(r"```(?:sql)?\s*(.*?)```", t, flags=re.S | re.I)
        if m:
            t = m.group(1)
        t = t.replace("```", "").strip()
        t = re.sub(r"^\s*sql\s*[:\-]?\s*", "", t, flags=re.I)
        return t

    logger.info("üîç run_analytics_query_on_database: start host=%s port=%s db=%s user=%s", host, port, database, user)

    # 1) Get schema and metadata
    pg = PostgresClient(host, port, user, password, database)
    await pg.init()
    try:
        # Get basic table->column structure
        schema = await pg.list_keys()
        
        # Get detailed metadata for schema context
        metadata = await pg.get_column_metadata()
        
        logger.info("üìä Retrieved %d tables and %d column metadata entries for SQL building", 
                   len(schema), len(metadata))
        
        # Build schema-aware context for LLM
        enhanced_schema = {}
        for table, columns in schema.items():
            # Find schema for this table
            table_schema = "public"  # default
            for meta_key, meta_data in metadata.items():
                if meta_data["table_name"] == table:
                    table_schema = meta_data["table_schema"]
                    break
            
            # Use schema.table as key and include type information
            schema_table_key = f"{table_schema}.{table}"
            column_details = []
            
            for col in columns:
                # Find column type from metadata
                col_type = "unknown"
                full_meta_key = f"{table_schema}.{table}.{col}"
                if full_meta_key in metadata:
                    col_type = metadata[full_meta_key]["data_type"]
                
                column_details.append(f"{col} ({col_type})")
            
            enhanced_schema[schema_table_key] = column_details
        
    except Exception as e:
        logger.exception("‚ùå DB schema introspection failed: %s", e)
        await pg.close()
        raise

    # 2) Enhanced context with schema and type information
    context_parts = [
        f"PostgreSQL database '{database}' schema with types and schema qualifiers:",
        json.dumps(enhanced_schema, indent=2),
        "",
        f"CRITICAL: You are working within a SINGLE database named '{database}'.",
        "The format 'schema.table' refers to SCHEMA.TABLE within the same database, NOT different databases.",
        "Example valid PostgreSQL queries:",
        "- SELECT * FROM public.users;",
        "- SELECT * FROM analytics.sales JOIN public.users ON sales.user_id = users.id;",
        "- SELECT count(*) FROM inventory.products;",
        "",
        "NEVER use database.table format - only use schema.table format.",
        "All tables shown are within the same PostgreSQL database.",
        "Column types are shown in parentheses for reference."
    ]
    context = "\n".join(context_parts)
    logger.info("üì§ Enhanced context for LLM: %d chars", len(context))

    # 3) LLM
    sql_raw = await llm.call_remote_llm(
        context=context,
        prompt=analytics_prompt,
        system_prompt=system_prompt
    )
    logger.info("Generated SQL (raw) length=%d", len(sql_raw or 0))
    sql = _strip_sql_fences(sql_raw)
    logger.info("üîç Generated SQL (full query):\n%s", sql)

    # 4) Execute with proper error handling
    rows = []
    try:
        rows = await pg.execute_query(sql)
        logger.info("‚úÖ Query executed successfully: rows=%d", len(rows or []))
        if rows:
            logger.debug("First row sample=%s", rows[0])
    except Exception as query_error:
        logger.error("‚ùå SQL execution failed: %s", str(query_error))
        logger.error("Failed SQL query was:\n%s", sql)
    finally:
        logger.debug("Closing DB pool ‚Ä¶")
        await pg.close()

    return {"rows": rows, "sql": sql}


@mcp.tool()
async def describe_columns(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    table: str,
    columns: List[str],
    limit: int,
    database_type: str = "postgres"
) -> str:
    """
    Describe only the given `columns` of `table` (or view) for Postgres or MSSQL.

    1. Connects to the correct database type (PostgresClient or MSSQLClient).
    2. Gets column metadata including data types and schema information.
    3. Samples up to `limit` values per column.
    4. Asks the LLM to produce a one-line description for each (excluding type info).

    Returns a JSON‚Äêencoded array of:
      [{ 
        "column": "table.col", 
        "description": "...", 
        "type": "data_type",
        "schema": "schema_name",
        "owner": "",  # Empty for Confluence sync compatibility
        "values": [...] 
      }, ‚Ä¶]

    Raises:
      ValueError: If `database_type` is unsupported.
    """
    logger.info("üîç describe_columns called for table/view '%s' in %s:%s/%s (%s)", 
               table, host, port, database, database_type)
    
    # 1) Pick the right client
    if database_type == "postgres":
        client = PostgresClient(
            host, port, user, password,
            database=database, min_size=1, max_size=5
        )
    elif database_type == "mssql":
        client = MSSQLClient(
            host, port, user, password,
            database=database
        )
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    # 2) Initialize
    await client.init()

    try:
        # First get the schema for this table
        schema_name = await client.get_table_schema(table)
        logger.info("üìÇ Using schema '%s' for table '%s'", schema_name, table)
        
        # Get comprehensive column metadata
        metadata = await client.get_column_metadata(schema=schema_name)
        logger.info("üìä Retrieved metadata for %d columns in schema '%s'", len(metadata), schema_name)
        
        results: List[Dict[str, Any]] = []
        for col in columns:
            try:
                # Look for column metadata using various key formats
                full_key = f"{schema_name}.{table}.{col}"
                metadata_entry = metadata.get(full_key)
                
                if not metadata_entry:
                    # Try without schema if not found
                    alt_keys = [key for key in metadata.keys() 
                              if key.endswith(f".{table}.{col}")]
                    if alt_keys:
                        metadata_entry = metadata[alt_keys[0]]
                        logger.debug("Found metadata using alternate key: %s", alt_keys[0])
                
                # Extract type and schema information
                data_type = metadata_entry.get("data_type", "unknown") if metadata_entry else "unknown"
                column_schema = metadata_entry.get("table_schema", schema_name) if metadata_entry else schema_name
                
                # Get sample values
                vals = await client.get_column_values(table, col, limit)
                
                # Create LLM prompt with schema context and type exclusion
                prompt = (
                    f"Describe the purpose and meaning of column '{col}' from table '{table}' "
                    f"in schema '{column_schema}'. "
                    f"Here are up to {limit} example values: {vals}. "
                    f"The column type is {data_type}. "
                    f"Provide only a brief functional description of what this column represents - "
                    f"do NOT include the data type in your description as that will be stored separately. "
                    f"Focus on the business meaning and purpose."
                )
                
                desc = await llm.call_remote_llm(
                    context=f"Database schema context: {column_schema}.{table}",
                    prompt=prompt
                )
                
                results.append({
                    "column":      f"{table}.{col}",
                    "description": desc.strip(),
                    "type":        data_type,
                    "schema":      column_schema,
                    "owner":       "",  # Empty owner field for Confluence sync compatibility
                    "values":      vals
                })
                
                logger.debug("‚úÖ Processed column %s.%s: type=%s, schema=%s", 
                           table, col, data_type, column_schema)
                           
            except Exception as inner:
                logger.error(
                    "‚ùå describe_columns error for %s.%s: %s",
                    table, col, inner, exc_info=True
                )
                results.append({
                    "column":      f"{table}.{col}",
                    "description": f"<error: {inner}>",
                    "type":        "unknown",
                    "schema":      schema_name,
                    "owner":       "",  # Empty owner field for Confluence sync compatibility
                    "values":      []
                })

        logger.info("‚úÖ Successfully processed %d columns with metadata", len(results))
        logger.debug("Sample result: %r", results[0] if results else None)
        return json.dumps(results, default=str, separators=(",",":"))

    finally:
        await client.close()


@mcp.tool()
async def analyze_dbt_file_for_iterative_query(
    dbt_file_content: str,
    host: str,
    port: str,
    user: str,
    password: str,
    database: str,
    analytics_prompt: str,
    confluence_space: str,
    confluence_title: str,
    database_type: str = "postgres"
) -> Dict[str, Any]:
    """
    Analyze a dbt file and iteratively build SQL queries starting from the highest depth tables.
    
    This is the main orchestration tool that:
    1. Parses the dbt file to extract tables/views by depth
    2. Starts with the highest depth tables
    3. Gets column metadata for current depth tables
    4. Asks AI if the available tables/columns are sufficient for the query
    5. If AI says "no", reduces depth by 1 and tries again
    6. If AI says "yes", generates and executes the full SQL query
    7. Continues until successful or reaches depth 0
    
    Args:
        dbt_file_content (str): JSON content of the dbt file
        host (str): Database host
        port (int): Database port  
        user (str): Database username
        password (str): Database password
        database (str): Database name
        analytics_prompt (str): The user's analytics question
        confluence_space (str): Confluence space for context
        confluence_title (str): Confluence page title for context
        database_type (str): Database type (default: postgres)
    
    Returns:
        Dict[str, Any]: Complete result including final SQL, data, and process log
    """
    logger.info("üöÄ Starting iterative dbt query analysis")
    logger.info(f"üìä Target database: {host}:{port}/{database}")
    logger.info(f"üéØ Analytics prompt: {analytics_prompt[:100]}...")
    logger.info(f"üìã Confluence context: {confluence_space}/{confluence_title}")
    
    try:
        # Step 1: Parse dbt file content
        logger.info("üìÑ Step 1: Parsing dbt file content")
        try:
            dbt_data = json.loads(dbt_file_content)
            logger.info(f"‚úÖ Successfully parsed dbt JSON file")
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Failed to parse dbt JSON: {e}")
            return {"status": "error", "error": f"Invalid JSON in dbt file: {e}"}
        
        # Step 2: Dynamically analyze dbt structure and extract depth information
        logger.info("üîç Step 2: Dynamically analyzing dbt structure for table depths")
        tables_by_depth, max_depth, dbt_context = await _analyze_dbt_structure_dynamic(dbt_data)
        
        if max_depth < 0:
            logger.error("‚ùå No valid table structure found in dbt file")
            return {
                "status": "error", 
                "error": "No valid table structure found in dbt file",
                "dbt_context": dbt_context
            }
        
        logger.info(f"üìä Found {sum(len(tables) for tables in tables_by_depth.values())} tables")
        logger.info(f"üèîÔ∏è  Maximum depth detected: {max_depth}")
        logger.info(f"üìã dbt Context: {dbt_context['description']}")
        
        for depth in sorted(tables_by_depth.keys()):
            count = len(tables_by_depth[depth])
            logger.info(f"   Depth {depth}: {count} tables")
        
        # Step 3: Start iterative process from max depth
        current_depth = max_depth
        process_log = []
        
        while current_depth >= 0:
            logger.info(f"üîÑ Step 3.{max_depth - current_depth + 1}: Trying depth {current_depth}")
            
            # Get tables from current depth only (not cumulative)
            current_tables = tables_by_depth.get(current_depth, [])
            
            if not current_tables:
                logger.info(f"‚è≠Ô∏è  No tables at depth {current_depth}, moving to next depth")
                current_depth -= 1
                continue
            
            logger.info(f"üìã Using {len(current_tables)} tables at depth {current_depth}")
            logger.debug(f"üè∑Ô∏è  Tables in scope: {current_tables}")
            
            try:
                # Get column metadata for current tables
                logger.info("üîç Getting column metadata for current table set")
                column_metadata = await get_database_column_metadata(
                    host=host, port=int(port), user=user, password=password,
                    database=database, database_type=database_type
                )
                
                # Filter metadata to only include our current tables
                filtered_metadata = {}
                for key, meta in column_metadata.items():
                    table_schema = meta.get("table_schema", "public")
                    table_name = meta.get("table_name")
                    
                    # Check if this table is in our current scope
                    if table_name in current_tables or f"{table_schema}.{table_name}" in current_tables:
                            filtered_metadata[key] = meta
                            break
                
                logger.info(f"üìä Found metadata for {len(filtered_metadata)} columns across {len(current_tables)} tables")
                
                # Ask AI if this table set is sufficient
                logger.info("ü§ñ Asking AI if current table set is sufficient")
                decision = await ask_ai_sufficiency_decision(
                    tables=current_tables,
                    column_metadata=filtered_metadata,
                    analytics_prompt=analytics_prompt,
                    current_depth=current_depth,
                    max_depth=max_depth,
                    dbt_context=dbt_context
                )
                
                process_log.append({
                    "depth": current_depth,
                    "table_count": len(current_tables),
                    "column_count": len(filtered_metadata),
                    "ai_decision": decision["decision"],
                    "ai_reasoning": decision.get("reasoning", "")
                })
                
                if decision["decision"].lower().strip() == "yes":
                    logger.info(f"‚úÖ AI says YES at depth {current_depth}! Proceeding with enhanced analysis")
                    
                    # Step A: Get filtered column keys for approved tables
                    logger.info("ÔøΩ Step A: Getting filtered database keys for approved tables")
                    try:
                        approved_keys = await list_database_keys_filtered_by_depth(
                            host=host, port=int(port), user=user, password=password,
                            database=database, approved_tables=current_tables,
                            database_type=database_type
                        )
                        logger.info(f"‚úÖ Retrieved keys for {len(approved_keys)} approved tables")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Failed to get filtered keys: {e}")
                        approved_keys = {}
                    
                    # Step B: Execute analytics query with approved tables only
                    logger.info("üìä Step B: Executing analytics query on approved tables")
                    try:
                        analytics_result = await run_analytics_query_on_approved_tables(
                            host=host, port=int(port), user=user, password=password,
                            database=database, analytics_prompt=analytics_prompt,
                            approved_tables=current_tables, database_type=database_type,
                            confluence_space=confluence_space, confluence_title=confluence_title
                        )
                        logger.info("‚úÖ Analytics query completed successfully")
                    except Exception as e:
                        logger.error(f"‚ùå Analytics query failed: {e}")
                        analytics_result = {"error": str(e)}
                    
                    # Return comprehensive results
                    return {
                        "status": "success",
                        "final_depth": current_depth,
                        "max_depth": max_depth,
                        "approved_tables": current_tables,
                        "column_count": len(filtered_metadata),
                        "process_log": process_log,
                        "approved_table_keys": approved_keys,
                        "analytics_result": analytics_result,
                        "sql_query": analytics_result.get("sql", ""),
                        "rows": analytics_result.get("rows", []),
                        "row_count": len(analytics_result.get("rows", [])),
                        "iteration_count": max_depth - current_depth + 1,
                        "dbt_context": dbt_context,
                        "filtering_applied": True
                    }
                
                else:
                    logger.warning(f"‚ùå AI says NO at depth {current_depth}: {decision.get('reasoning', 'No reason provided')}")
                    
                    if current_depth == 0:
                        logger.error("üö´ Reached depth 0 and AI still says NO - cannot proceed")
                        return {
                            "status": "insufficient_data",
                            "error": "AI could not generate query even with all available tables (depth 0)",
                            "final_depth": 0,
                            "max_depth": max_depth,
                            "process_log": process_log,
                            "iteration_count": max_depth + 1
                        }
                    
                    # Reduce depth and try again
                    current_depth -= 1
                    logger.info(f"‚¨áÔ∏è  Reducing depth to {current_depth} and retrying")
                    
            except Exception as depth_error:
                logger.error(f"‚ùå Error at depth {current_depth}: {depth_error}", exc_info=True)
                process_log.append({
                    "depth": current_depth,
                    "error": str(depth_error),
                    "table_count": len(current_tables) if 'current_tables' in locals() else 0
                })
                
                if current_depth == 0:
                    return {
                        "status": "error",
                        "error": f"Failed at all depth levels. Final error: {depth_error}",
                        "process_log": process_log
                    }
                
                current_depth -= 1
                logger.warning(f"‚¨áÔ∏è  Error occurred, reducing depth to {current_depth}")
        
        # Should never reach here, but just in case
        logger.error("üö´ Exhausted all depth levels without success")
        return {
            "status": "exhausted",
            "error": "Tried all depth levels from max to 0 without success",
            "process_log": process_log
        }
        
    except Exception as e:
        logger.error(f"‚ùå Fatal error in iterative analysis: {e}", exc_info=True)
        return {
            "status": "error",
            "error": f"Fatal error: {e}"
        }


async def _analyze_dbt_structure_dynamic(dbt_data: Dict[str, Any]) -> Tuple[Dict[int, List[str]], int, Dict[str, Any]]:
    """
    Dynamically analyze dbt file structure to extract tables by depth and determine context.
    
    Returns:
        - tables_by_depth: Dict mapping depth level to list of table names
        - max_depth: Maximum depth found in the structure  
        - dbt_context: Dictionary with file type, description, and metadata
    """
    tables_by_depth: Dict[int, List[str]] = {}
    max_depth = -1
    
    # Initialize context information
    dbt_context = {
        "type": "Unknown dbt file",
        "description": "dbt configuration",
        "total_tables": 0,
        "depth_distribution": {}
    }
    
    try:
        # Case 1: Explicit depth structure (like stadium.json)
        if isinstance(dbt_data, dict) and any(key.startswith("depth") for key in dbt_data.keys()):
            logger.info("üîç Found explicit depth structure")
            
            for key, value in dbt_data.items():
                if key.startswith("depth") and isinstance(value, list):
                    try:
                        depth_num = int(re.search(r'\d+', key).group())
                        tables_by_depth[depth_num] = value
                        max_depth = max(max_depth, depth_num)
                    except (AttributeError, ValueError):
                        pass
            
            total_tables = sum(len(tables) for tables in tables_by_depth.values())
            dbt_context.update({
                "type": "depth-organized structure",
                "description": f"depth-organized structure with {total_tables} tables across {max_depth + 1} depth levels",
                "total_tables": total_tables
            })
        
        # Case 2: dbt relations structure (manifest-like)
        elif isinstance(dbt_data, dict) and "relations" in dbt_data:
            logger.info("üîç Found dbt relations structure")
            relations = dbt_data["relations"]
            
            # Store full relation metadata while extracting depth info
            relation_metadata = []
            
            for relation in relations:
                # Preserve ALL permanent keys in the relation
                relation_copy = dict(relation)  # Keep all original keys
                
                # Extract key information for depth analysis
                depth = relation.get("depth", 0)
                table_name = relation.get("identifier", relation.get("name", relation.get("table_name", "unknown")))
                
                # If no explicit depth, infer from table name
                if depth == 0 and table_name != "unknown":
                    depth = _infer_table_depth_dynamic(str(table_name))
                    relation_copy["inferred_depth"] = depth  # Mark as inferred
                
                # Store in depth mapping
                if depth not in tables_by_depth:
                    tables_by_depth[depth] = []
                tables_by_depth[depth].append(table_name)
                max_depth = max(max_depth, depth)
                
                # Preserve full relation metadata
                relation_metadata.append(relation_copy)
            
            total_tables = sum(len(tables) for tables in tables_by_depth.values())
            dbt_context.update({
                "type": "dbt relations",
                "description": f"dbt relations structure with {total_tables} relations across {max_depth + 1} depth levels",
                "total_tables": total_tables,
                "relations_metadata": relation_metadata,  # Preserve ALL relation data
                "permanent_keys_preserved": True
            })
        
        # Case 3: dbt models file
        elif isinstance(dbt_data, dict) and ("models" in dbt_data or "model" in dbt_data):
            logger.info("üîç Found dbt models structure")
            models = dbt_data.get("models", dbt_data.get("model", {}))
            
            if isinstance(models, dict):
                tables_list = list(models.keys())
            else:
                tables_list = []
            
            # Infer depths from model names
            for table in tables_list:
                depth = _infer_table_depth_dynamic(str(table))
                if depth not in tables_by_depth:
                    tables_by_depth[depth] = []
                tables_by_depth[depth].append(table)
                max_depth = max(max_depth, depth)
            
            dbt_context.update({
                "type": "dbt models",
                "description": f"dbt models configuration with {len(tables_list)} models",
                "total_tables": len(tables_list)
            })
        
        # Case 4: dbt sources file
        elif "sources" in dbt_data:
            logger.info("üîç Found dbt sources structure")
            sources = dbt_data["sources"]
            
            tables_list = []
            if isinstance(sources, dict):
                for source_name, source_data in sources.items():
                    if isinstance(source_data, dict) and "tables" in source_data:
                        tables_list.extend(source_data["tables"])
            
            # Infer depths from source table names
            for table in tables_list:
                depth = _infer_table_depth_dynamic(str(table))
                if depth not in tables_by_depth:
                    tables_by_depth[depth] = []
                tables_by_depth[depth].append(table)
                max_depth = max(max_depth, depth)
            
            dbt_context.update({
                "type": "dbt sources",
                "description": f"dbt sources configuration with {len(tables_list)} source tables",
                "total_tables": len(tables_list)
            })
        
        # Case 5: dbt manifest file  
        elif "nodes" in dbt_data:
            logger.info("üîç Found dbt manifest structure")
            nodes = dbt_data["nodes"]
            
            tables_list = []
            if isinstance(nodes, dict):
                for node_name, node_data in nodes.items():
                    if isinstance(node_data, dict) and node_data.get("resource_type") == "model":
                        table_name = node_data.get("name", node_name.split(".")[-1])
                        tables_list.append(table_name)
            
            # Infer depths from node names
            for table in tables_list:
                depth = _infer_table_depth_dynamic(str(table))
                if depth not in tables_by_depth:
                    tables_by_depth[depth] = []
                tables_by_depth[depth].append(table)
                max_depth = max(max_depth, depth)
            
            dbt_context.update({
                "type": "dbt manifest",
                "description": f"dbt manifest with {len(tables_list)} model nodes",
                "total_tables": len(tables_list)
            })
        
        # Case 6: Generic array or object
        else:
            logger.info("üîç Analyzing generic structure")
            tables_list = []
            
            if isinstance(dbt_data, list):
                # Array format
                for item in dbt_data:
                    if isinstance(item, dict):
                        name = item.get("name") or item.get("table_name") or item.get("table")
                        if name:
                            tables_list.append(name)
                    elif isinstance(item, str):
                        tables_list.append(item)
                
                dbt_context.update({
                    "type": "table list",
                    "description": f"table list with {len(tables_list)} entries"
                })
            
            elif isinstance(dbt_data, dict):
                # Generic object - look for table information
                for key, value in dbt_data.items():
                    if isinstance(value, list):
                        tables_list.extend([str(item) for item in value if isinstance(item, (str, int))])
                    elif isinstance(value, dict):
                        for subkey, subvalue in value.items():
                            if isinstance(subvalue, list):
                                tables_list.extend([str(item) for item in subvalue if isinstance(item, (str, int))])
                
                dbt_context.update({
                    "type": "generic structure",
                    "description": f"generic structure with {len(tables_list)} extracted table references"
                })
            
            # Infer depths for generic structures
            for table in tables_list:
                depth = _infer_table_depth_dynamic(str(table))
                if depth not in tables_by_depth:
                    tables_by_depth[depth] = []
                tables_by_depth[depth].append(table)
                max_depth = max(max_depth, depth)
            
            dbt_context["total_tables"] = len(tables_list)
        
        # Update final context
        dbt_context["depth_distribution"] = {f"depth_{k}": len(v) for k, v in tables_by_depth.items()}
        
        logger.info(f"üìä Final analysis: {dbt_context['description']}")
        logger.info(f"üéØ Depth distribution: {dbt_context['depth_distribution']}")
        
        return tables_by_depth, max_depth, dbt_context
        
    except Exception as e:
        logger.error(f"‚ùå Error analyzing dbt structure: {e}")
        return {}, -1, dbt_context


def _infer_table_depth_dynamic(table_name: str) -> int:
    """
    Dynamically infer table depth from naming patterns and conventions.
    
    Higher numbers = more detailed/specific tables
    Lower numbers = more aggregated/summary tables
    """
    table_lower = table_name.lower()
    
    # Fact tables and detailed transaction tables (highest depth)
    if any(keyword in table_lower for keyword in [
        'fact_', 'facts_', 'transaction', 'detail', 'raw_', 'staging_', 'stg_', 'src_'
    ]):
        return 4
    
    # Dimension tables and reference data
    elif any(keyword in table_lower for keyword in [
        'dim_', 'dimension', 'ref_', 'lookup', 'master', 'bridge_'
    ]):
        return 3
    
    # Aggregated/summary tables
    elif any(keyword in table_lower for keyword in [
        'agg_', 'summary', 'sum_', 'rollup', 'daily', 'monthly', 'weekly', 'mart_'
    ]):
        return 2
    
    # High-level summary/dashboard tables
    elif any(keyword in table_lower for keyword in [
        'dashboard', 'kpi', 'metric', 'report', 'executive', 'overview'
    ]):
        return 1
    
    # Default depth for unclassified tables (middle ground)
    else:
        return 2


@mcp.tool()
async def ask_ai_sufficiency_decision(
    tables: List[str],
    column_metadata: Dict[str, Dict[str, Any]], 
    analytics_prompt: str,
    current_depth: int,
    max_depth: int,
    dbt_context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Ask AI if the current set of tables and columns is sufficient to answer the analytics prompt.
    
    This tool presents the AI with:
    1. The available tables and their metadata
    2. The user's analytics question
    3. Context about the current depth level and dbt structure
    
    The AI must respond with exactly "yes" or "no" plus reasoning.
    
    Args:
        tables (List[str]): Current table names available at this depth level
        column_metadata (Dict): Column metadata for the current tables
        analytics_prompt (str): The user's analytics question
        current_depth (int): Current depth level being evaluated
        max_depth (int): Maximum depth available in the dataset
        dbt_context (Dict): Dynamic context about the dbt file structure
    
    Returns:
        Dict[str, Any]: AI decision (yes/no) and reasoning
    """
    logger.debug(f"ü§ñ AI Sufficiency Check - Depth {current_depth}")
    logger.debug(f"üìä Evaluating {len(tables)} tables with {len(column_metadata)} columns")
    
    # Build table summary for AI
    table_summary = []
    for table_name in tables:
        table_columns = []
        
        for col_key, col_meta in column_metadata.items():
            if col_meta.get("table_name") == table_name:
                table_columns.append({
                    "name": col_meta.get("column_name"),
                    "type": col_meta.get("data_type"),
                    "nullable": col_meta.get("is_nullable", "unknown")
                })
        
        table_summary.append({
            "table": table_name,
            "columns": table_columns,
            "column_count": len(table_columns)
        })
    
    logger.debug(f"üìã Table summary prepared with {sum(len(t['columns']) for t in table_summary)} total columns")
    
    # Create AI prompt for yes/no decision  
    context = f"""{dbt_context['description']} - Depth Level {current_depth} of {max_depth}

You are evaluating whether the current set of tables and columns is sufficient to answer a user's analytics question.

AVAILABLE TABLES AND COLUMNS:
{json.dumps(table_summary, indent=2)}

DEPTH CONTEXT:
- Current depth: {current_depth} (0 = base tables, higher = more derived/aggregated tables)  
- Maximum depth: {max_depth}
- Tables available: {len(tables)}
- Total columns: {len(column_metadata)}
- Database context: {dbt_context['description']}

USER'S ANALYTICS QUESTION:
{analytics_prompt}

DECISION REQUIRED:
You must answer with EXACTLY "yes" or "no" followed by your reasoning.

- Answer "yes" if the available tables and columns provide sufficient data to answer the user's question completely and accurately
- Answer "no" if you need more fundamental/base tables (lower depth) to properly answer the question

Your response format must be:
DECISION: yes
REASONING: [your explanation]

OR

DECISION: no  
REASONING: [your explanation]"""

    try:
        logger.debug("üì§ Sending decision request to LLM")
        ai_response = await llm.call_remote_llm(
            context=context,
            prompt="Evaluate if the current tables are sufficient for the analytics question. Respond with DECISION: yes/no and REASONING:",
            system_prompt="You are a database analyst. Evaluate table sufficiency and respond with exactly 'DECISION: yes' or 'DECISION: no' followed by 'REASONING: [explanation]'."
        )
        
        logger.debug(f"üì• AI response received: {ai_response[:200]}...")
        
        # Parse AI response
        decision = "no"  # Default to no
        reasoning = "Could not parse AI response"
        
        if ai_response:
            lines = ai_response.strip().split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith("DECISION:"):
                    decision_part = line.replace("DECISION:", "").strip().lower()
                    if decision_part in ["yes", "no"]:
                        decision = decision_part
                elif line.startswith("REASONING:"):
                    reasoning = line.replace("REASONING:", "").strip()
        
        logger.info(f"üéØ AI Decision: {decision.upper()}")
        logger.info(f"üí≠ AI Reasoning: {reasoning}")
        
        return {
            "decision": decision,
            "reasoning": reasoning,
            "raw_response": ai_response,
            "depth_evaluated": current_depth,
            "tables_count": len(tables),
            "columns_count": len(column_metadata)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting AI decision: {e}", exc_info=True)
        return {
            "decision": "no",
            "reasoning": f"Error occurred during AI evaluation: {e}",
            "error": str(e),
            "depth_evaluated": current_depth
        }


@mcp.tool()
async def list_database_keys_filtered_by_depth(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    approved_tables: List[str],
    database_type: str = "postgres"
) -> Dict[str, List[str]]:
    """
    List column names ("keys") only for tables that are in the approved depth list.
    
    This is a filtered version of list_database_keys that only returns columns
    for tables that have been approved in the iterative dbt analysis.
    
    Args:
      host (str):          DB host.
      port (int):          DB port.
      user (str):          Username.
      password (str):      Password.
      database (str):      Database to inspect.
      approved_tables (List[str]): List of table names that were approved in iterative analysis.
      database_type (str): Either "postgres" or "mssql".

    Returns:
      Dict[str, List[str]]:
        e.g. {
          "approved_table1": ["col1", "col2", ...],
          "approved_table2": ["col1", "col2", ...]
        }
        Only includes tables from approved_tables list.
    """
    logger.info(
        f"üîç list_database_keys_filtered_by_depth called for {len(approved_tables)} approved tables: {approved_tables}"
    )

    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        # Get all keys first
        all_keys = await client.list_keys()
        
        # Filter to only include approved tables
        filtered_keys = {}
        approved_set = set(approved_tables)
        
        for table_name, columns in all_keys.items():
            if table_name in approved_set:
                filtered_keys[table_name] = columns
                logger.info(f"‚úÖ Included table '{table_name}' with {len(columns)} columns")
            else:
                logger.debug(f"üö´ Skipped table '{table_name}' (not in approved list)")
        
        logger.info(f"üìä Filtered results: {len(filtered_keys)} tables from {len(all_keys)} total")
        return filtered_keys
        
    except Exception as e:
        logger.error(f"‚ùå Error in list_database_keys_filtered_by_depth: {e}", exc_info=True)
        raise
    finally:
        await client.close()


@mcp.tool()
async def run_analytics_query_on_approved_tables(
    host: str,
    port: int,
    user: str,
    password: str,
    database: str,
    analytics_prompt: str,
    approved_tables: List[str],
    database_type: str = "postgres",
    confluence_space: str = "",
    confluence_title: str = ""
) -> Dict[str, Any]:
    """
    Execute an analytics query only on tables that are in the approved depth list.
    
    This is a filtered version of run_analytics_query_on_database that only considers
    tables that have been approved in the iterative dbt analysis.
    
    Args:
      host (str):          DB host.
      port (int):          DB port.
      user (str):          Username.
      password (str):      Password.
      database (str):      Database to inspect.
      analytics_prompt (str): The analytics question/prompt.
      approved_tables (List[str]): List of table names that were approved in iterative analysis.
      database_type (str): Either "postgres" or "mssql".
      confluence_space (str): Confluence space for context (optional).
      confluence_title (str): Confluence page title for context (optional).

    Returns:
      Dict[str, Any]: Query results including SQL, data, and metadata.
    """
    logger.info(
        f"üîç run_analytics_query_on_approved_tables called for {len(approved_tables)} approved tables"
    )
    logger.info(f"üìä Analytics prompt: {analytics_prompt[:100]}...")
    logger.info(f"‚úÖ Approved tables: {approved_tables}")

    if database_type == "postgres":
        client = PostgresClient(host, port, user, password,
                                database=database, min_size=1, max_size=5)
    elif database_type == "mssql":
        client = MSSQLClient(host, port, user, password,
                             database=database)
    else:
        raise ValueError(f"Unsupported database_type: {database_type!r}")

    await client.init()
    try:
        # Get schema information only for approved tables
        logger.info("üîç Getting schema information for approved tables...")
        
        # Get all tables/views first
        all_tables = await client.list_tables_and_views()
        
        # Filter to only approved tables
        approved_schemas = []
        approved_set = set(approved_tables)
        
        for table_info in all_tables:
            table_name = table_info.get("table_name", table_info.get("name", ""))
            if table_name in approved_set:
                approved_schemas.append(table_info)
                logger.info(f"‚úÖ Included table schema for '{table_name}'")
            else:
                logger.debug(f"üö´ Skipped table schema for '{table_name}' (not approved)")
        
        logger.info(f"üìä Filtered schema: {len(approved_schemas)} tables from {len(all_tables)} total")
        
        # Get column information only for approved tables
        logger.info("üîç Getting column information for approved tables...")
        approved_columns = []
        
        for table_info in approved_schemas:
            table_name = table_info.get("table_name", table_info.get("name", ""))
            schema_name = table_info.get("schema", "public")
            
            try:
                columns = await client.list_columns(table_name, schema_name)
                for col in columns:
                    col["table_name"] = table_name  # Ensure table name is included
                    approved_columns.append(col)
                
                logger.info(f"‚úÖ Got {len(columns)} columns for table '{table_name}'")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to get columns for table '{table_name}': {e}")
        
        logger.info(f"üìä Total approved columns: {len(approved_columns)}")
        
        # Build enhanced schema context only with approved tables
        enhanced_schema_context = ""
        if confluence_space and confluence_title:
            try:
                enhanced_result = await get_enhanced_schema_with_confluence(
                    host, port, user, password, database, 
                    confluence_space, confluence_title, database_type
                )
                
                if enhanced_result and "content" in enhanced_result:
                    enhanced_content = enhanced_result["content"][0]["text"]
                    # Parse and filter the enhanced schema to only include approved tables
                    try:
                        enhanced_data = json.loads(enhanced_content)
                        if "tables" in enhanced_data:
                            filtered_tables = []
                            for table in enhanced_data["tables"]:
                                table_name = table.get("name", "")
                                if table_name in approved_set:
                                    filtered_tables.append(table)
                            
                            enhanced_data["tables"] = filtered_tables
                            enhanced_schema_context = json.dumps(enhanced_data, indent=2)
                            logger.info(f"‚úÖ Filtered enhanced schema to {len(filtered_tables)} approved tables")
                    except json.JSONDecodeError:
                        logger.warning("‚ö†Ô∏è Could not parse enhanced schema for filtering")
                        enhanced_schema_context = enhanced_content
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to get enhanced schema context: {e}")
        
        # Build the analytics prompt with only approved table context
        filtered_context = {
            "approved_tables": approved_tables,
            "table_schemas": approved_schemas,
            "column_details": approved_columns,
            "total_approved_tables": len(approved_schemas),
            "total_approved_columns": len(approved_columns)
        }
        
        enhanced_prompt = f"""
{analytics_prompt}

CONTEXT (APPROVED TABLES ONLY):
You have access to {len(approved_schemas)} approved tables with {len(approved_columns)} total columns.

Approved Tables: {', '.join(approved_tables)}

Table Schemas:
{json.dumps(approved_schemas, indent=2)}

Column Details:
{json.dumps(approved_columns, indent=2)}

{f"Enhanced Documentation Context: {enhanced_schema_context}" if enhanced_schema_context else ""}

IMPORTANT: Only use tables from the approved list: {approved_tables}
Do not reference any tables outside of this approved set.
"""
        
        # Execute the analytics query with filtered context
        logger.info("üöÄ Executing analytics query with approved tables context...")
        
        # Use LLM to generate SQL with the enhanced prompt
        sql_raw = await llm.call_remote_llm(
            context=enhanced_prompt,
            prompt="Generate SQL query based on the provided context and analytics request.",
            system_prompt="You are a PostgreSQL expert. Generate efficient SQL queries based on the provided database schema and requirements. Return only the SQL query without explanation."
        )
        
        # Clean the SQL (strip code fences)
        def _strip_sql_fences_local(txt: str) -> str:
            if not txt:
                return txt
            t = re.sub(r"</?code[^>]*>", "", txt, flags=re.I)
            m = re.search(r"```(?:sql)?\s*(.*?)```", t, flags=re.S | re.I)
            if m:
                t = m.group(1)
            t = t.replace("```", "").strip()
            return t
        
        sql = _strip_sql_fences_local(sql_raw)
        logger.info("üîç Generated SQL for approved tables:\n%s", sql)
        
        # Execute the query
        rows = []
        try:
            rows = await client.execute_query(sql)
            logger.info("‚úÖ Query executed successfully: rows=%d", len(rows or []))
            if rows:
                logger.debug("First row sample=%s", rows[0])
        except Exception as query_error:
            logger.error("‚ùå SQL execution failed: %s", str(query_error))
            logger.error("Failed SQL query was:\n%s", sql)
            # Return error result instead of raising
            return {
                "error": str(query_error),
                "sql": sql,
                "rows": [],
                "filtering_info": {
                    "approved_tables": approved_tables,
                    "total_approved_tables": len(approved_schemas),
                    "total_approved_columns": len(approved_columns),
                    "filtering_applied": True
                }
            }
        
        # Build result with filtering metadata
        result = {
            "rows": rows, 
            "sql": sql,
            "filtering_info": {
                "approved_tables": approved_tables,
                "total_approved_tables": len(approved_schemas),
                "total_approved_columns": len(approved_columns),
                "filtering_applied": True
            }
        }
        
        logger.info("‚úÖ Analytics query completed successfully with filtered context")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error in run_analytics_query_on_approved_tables: {e}", exc_info=True)
        raise
    finally:
        await client.close()


if __name__ == "__main__":
    mcp.run(transport="streamable-http")
